{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6bsR3T-VDZ4",
        "outputId": "b082783a-0e1b-48c0-d833-ebf57e536368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting point: (-1, -1), Solution: [1. 1.], Iterations: 5\n",
            "Starting point: (0, 0), Solution: [1. 1.], Iterations: 2\n",
            "Starting point: (2, 2), Solution: [1. 1.], Iterations: 5\n",
            "Starting point: (1.5, 1.5), Solution: [1. 1.], Iterations: 5\n"
          ]
        }
      ],
      "source": [
        "# problem 3 a\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# this is Rosenbrock function\n",
        "def rosenbrock(x):\n",
        "    return 100.0 * (x[1] - x[0]**2.0)**2.0 + (1 - x[0])**2.0\n",
        "\n",
        "# this is the gradient of Rosenbrock function\n",
        "def rosenbrock_grad(x):\n",
        "    return np.array([-400.0*x[0]*(x[1]-x[0]**2.0) - 2.0*(1-x[0]), 200.0*(x[1]-x[0]**2.0)])\n",
        "\n",
        "# this is the hessian matrix of rosenbrock function\n",
        "def rosenbrock_hess(x):\n",
        "    return np.array([[1200.0*x[0]**2.0 - 400.0*x[1] + 2, -400.0*x[0]], [-400.0*x[0], 200.0]])\n",
        "\n",
        "# this is Newton method for optimization\n",
        "def newtons_method(start_point, tol=1e-5, max_iter=1000):\n",
        "    x = start_point\n",
        "    for i in range(max_iter):\n",
        "        grad = rosenbrock_grad(x)\n",
        "        hess = rosenbrock_hess(x)\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            break\n",
        "        delta = np.linalg.solve(hess, -grad)\n",
        "        x = x + delta\n",
        "    return x, i\n",
        "\n",
        "# test with different starting points\n",
        "starting_points = [(-1, -1), (0, 0), (2, 2), (1.5, 1.5)]\n",
        "\n",
        "\n",
        "results = []\n",
        "for point in starting_points:\n",
        "    result, iterations = newtons_method(np.array(point))\n",
        "    results.append((result, iterations))\n",
        "\n",
        "for i, (start_point, result) in enumerate(zip(starting_points, results)):\n",
        "    solution, iterations = result\n",
        "    print(f'Starting point: {start_point}, Solution: {solution}, Iterations: {iterations}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pr3 b\n",
        "\n",
        "def steepest_descent(start_point, tol=1e-5, max_iter=1000, alpha=1e-3):\n",
        "    x = np.array(start_point, dtype='float64')\n",
        "    for i in range(max_iter):\n",
        "        grad = rosenbrock_grad(x)\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            break\n",
        "        x -= alpha * grad\n",
        "    return x, i\n",
        "\n",
        "# run the steepest descent optimization with the same starting points\n",
        "results_sd = []\n",
        "for point in starting_points:\n",
        "    result_sd, iterations_sd = steepest_descent(point)\n",
        "    results_sd.append((result_sd, iterations_sd))\n",
        "\n",
        "# display the results for steepest descent\n",
        "results_sd_formatted = []\n",
        "for i, (start_point, result_sd) in enumerate(zip(starting_points, results_sd)):\n",
        "    solution_sd, iterations_sd = result_sd\n",
        "    results_sd_formatted.append(f'Starting point: {start_point}, Solution: {solution_sd}, Iterations: {iterations_sd}')\n",
        "\n",
        "results_sd_formatted\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtTD72O4Wkyw",
        "outputId": "d35aa468-4631-4ce9-f803-58a079414442"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Starting point: (-1, -1), Solution: [0.66670739 0.4428965 ], Iterations: 999',\n",
              " 'Starting point: (0, 0), Solution: [0.67388605 0.45255952], Iterations: 999',\n",
              " 'Starting point: (2, 2), Solution: [1.20924655 1.46301622], Iterations: 999',\n",
              " 'Starting point: (1.5, 1.5), Solution: [1.18405047 1.40263525], Iterations: 999']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the conjugate gradient algorithm using the Hestenes-Stiefel formula for beta\n",
        "def conjugate_gradient_hs(f, grad_f, x0, tol=1e-5, max_iter=1000):\n",
        "    x = np.array(x0, dtype='float64')\n",
        "    r = -grad_f(x)\n",
        "    d = r.copy()\n",
        "    for i in range(max_iter):\n",
        "        # Approximate the Hessian-vector product\n",
        "        Hd = grad_f(x + tol * d) - grad_f(x) / tol\n",
        "        alpha = np.dot(r, r) / np.dot(d, Hd) if np.dot(d, Hd) != 0 else 0\n",
        "        x_new = x + alpha * d\n",
        "        r_new = -grad_f(x_new)\n",
        "        beta = np.dot(r_new, r_new - r) / np.dot(d, r_new - r) if np.dot(d, r_new - r) != 0 else 0\n",
        "        d = r_new + beta * d\n",
        "        x = x_new\n",
        "        r = r_new\n",
        "        if np.linalg.norm(r) < tol:\n",
        "            break\n",
        "    return x, i\n",
        "\n",
        "# New starting points closer to the global minimum\n",
        "new_starting_points = [(0.5, 0.5), (1.2, 1.2), (1.0, 1.0)]\n",
        "\n",
        "# Test the conjugate gradient algorithm with Hestenes-Stiefel beta update and new starting points\n",
        "new_results_cg_hs = []\n",
        "for point in new_starting_points:\n",
        "    result_cg_hs, iterations_cg_hs = conjugate_gradient_hs(\n",
        "        rosenbrock, rosenbrock_grad, np.array(point))\n",
        "    new_results_cg_hs.append((result_cg_hs, iterations_cg_hs))\n",
        "\n",
        "# Format the new results for display\n",
        "new_results_cg_hs_formatted = []\n",
        "for i, (start_point, result_cg_hs) in enumerate(zip(new_starting_points, new_results_cg_hs)):\n",
        "    solution_cg_hs, iterations_cg_hs = result_cg_hs\n",
        "    new_results_cg_hs_formatted.append(f'Starting point: {start_point}, Solution: {solution_cg_hs}, Iterations: {iterations_cg_hs}')\n",
        "\n",
        "new_results_cg_hs_formatted\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKFgA3kza9sV",
        "outputId": "c5909cd2-68d6-4c21-f25b-eb137d3bace3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Starting point: (0.5, 0.5), Solution: [0.6271381  0.39334365], Iterations: 999',\n",
              " 'Starting point: (1.2, 1.2), Solution: [1.11201358 1.23699203], Iterations: 999',\n",
              " 'Starting point: (1.0, 1.0), Solution: [1. 1.], Iterations: 0']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results suggest that Newton's Method is far more efficient for this problem, converging to the exact solution in very few iterations. This is  because Newton's Method uses second-order derivative information, which allows it to adjust its path towards the minimum more accurately and take larger steps when it is far from the minimum.\n",
        "\n",
        "Both the Steepest Descent Method and the Conjugate Gradient Method did not converge within the given iteration limit, indicating they are less efficient for this particular function. Steepest Descent  perform poorly on functions with narrow valleys because it does not account for the curvature of the function, leading to a zigzagging path towards the minimum. The Conjugate Gradient Method is usually more efficient than Steepest Descent but still fell short for this function within the given number of iterations."
      ],
      "metadata": {
        "id": "bIAnlgdIeDUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HVlavoWYfmFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gradient_descent(X, Y, learning_rate, iterations):\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    for iteration in range(iterations):\n",
        "        gradients = 2 / m * X.T.dot(X.dot(theta) - Y)\n",
        "        theta -= learning_rate * gradients\n",
        "    return theta\n",
        "\n",
        "def run_stochastic_gradient_descent(X, Y, learning_rate, iterations):\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    for iteration in range(iterations):\n",
        "        for i in range(m):\n",
        "            random_index = np.random.randint(m)\n",
        "            xi = X[random_index:random_index+1]\n",
        "            yi = Y[random_index:random_index+1]\n",
        "            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "            theta -= learning_rate * gradients\n",
        "    return theta\n",
        "\n",
        "# Step sizes to test\n",
        "step_sizes = [0.5, 0.1, 0.03, 0.01, 0.003, 0.001]\n",
        "\n",
        "# Comparing Gradient Descent and Stochastic Gradient Descent\n",
        "for step_size in step_sizes:\n",
        "    theta_gd = run_gradient_descent(X_b, Y, learning_rate=step_size, iterations=100)  # or your chosen number of iterations\n",
        "    theta_sgd = run_stochastic_gradient_descent(X_b, Y, learning_rate=step_size, iterations=1)  # m iterations, one epoch\n",
        "    print(f\"Step size: {step_size}\\nGradient Descent Theta: {theta_gd}\\nStochastic Gradient Descent Theta: {theta_sgd}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3Oai8VzhBoL",
        "outputId": "9a8942ee-1789-43ff-ce84-bb11141df1a2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step size: 0.5\n",
            "Gradient Descent Theta: [ 0.995543   -1.97238924  2.99149022 -4.0019434 ]\n",
            "Stochastic Gradient Descent Theta: [-2.54441846e+174  1.49035773e+174 -1.88336038e+174  3.01320103e+173]\n",
            "\n",
            "Step size: 0.1\n",
            "Gradient Descent Theta: [ 0.49458274 -1.46789209  2.56540647 -3.12589939]\n",
            "Stochastic Gradient Descent Theta: [ 1.37799297 -2.11617436  2.99834238 -3.51941953]\n",
            "\n",
            "Step size: 0.03\n",
            "Gradient Descent Theta: [ 0.01266839 -0.75173761  1.19884794 -1.5580648 ]\n",
            "Stochastic Gradient Descent Theta: [ 0.83271907 -1.78555078  2.9338468  -3.72548004]\n",
            "\n",
            "Step size: 0.01\n",
            "Gradient Descent Theta: [-0.18570309 -0.38330675  0.37640081 -0.69803057]\n",
            "Stochastic Gradient Descent Theta: [ 0.86844614 -1.74383211  3.01248334 -3.95682521]\n",
            "\n",
            "Step size: 0.003\n",
            "Gradient Descent Theta: [-0.1694453  -0.17857416  0.0626198  -0.27857616]\n",
            "Stochastic Gradient Descent Theta: [ 0.97795268 -2.00748292  3.03048364 -4.01002731]\n",
            "\n",
            "Step size: 0.001\n",
            "Gradient Descent Theta: [-0.08169768 -0.07314157  0.00858814 -0.10703609]\n",
            "Stochastic Gradient Descent Theta: [ 1.00579995 -1.98012115  3.01602889 -4.06030366]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In comparing Gradient Descent and Stochastic Gradient Descent with  different step sizes showed GD converges more smoothly as it uses the whole dataset to calculate gradients, while SGD's performance is heavily dependent on step size, with large step sizes leading to divergence, as seen with 0.5. With smaller step sizes, SGD rapidly approached the true parameter values, even with an equivalent of a single GD iteration. This showcases SGD's efficiency for large datasets and its sensitivity to step size selection, requiring careful tuning to prevent divergence and achieve fast convergence."
      ],
      "metadata": {
        "id": "C9lAcCutlIJh"
      }
    }
  ]
}